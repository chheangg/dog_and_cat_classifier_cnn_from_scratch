{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b53324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528c1e5",
   "metadata": {},
   "source": [
    "# 2.0 Convolutional Layer\n",
    "The earliest breakthrough in pattern recognition comes from LeCun's Digit Recognition model [[1]](#ref1), which utilized Convolutional Layer to learn features. The model's implementation is based on Fukushima's Neocognitron architecture, which in turn is based on visual cortex of the human body. This mimicry shows that much can be learned from nature. \n",
    "\n",
    "At present (writing this as of Sep. 1st, 2025), a lot of computer vision model still utilize Convolutional layer though they are being displaced by Transformers. They are still the basis for most computer vision tasks due to their ability to extract local features and patterns efficiently compared to the approaches in the previous notebook. When we start to stack layers together to add complexity and also train on bigger images with more channels, the time and space requirements grow in order of magnitude rendering our earlier model ill-suited for the task. A Convolutional layer is much more efficient, it utilizes **weight-sharing** or a **kernel** due to **translation invariance** and **locality**.\n",
    "\n",
    "## 2.1. Convolutional Math\n",
    "The reader may heard of this from ODE (or taught it, if you're reading this Professor Lin!), which has the following form\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} f(x)g(t-x)dt\n",
    "$$\n",
    "What convolution does is that it slides one function over the other, by adding the product of where they overlap, to produce a final function. Any who, integrals are just Riemann sum, we can get away with a finite sum rather than one that goes to infinity (NOT SUBSTANTIATED, TRUST ME AT YOUR OWN PERIL!!!). \n",
    "$$\n",
    "\\sum_{i=-\\infty}^{\\infty} f(x)g(t-x)dt\n",
    "$$\n",
    "In the context of a Classification problem, we want our Convolution function to iterate over all part of an image and extract the important underlying features. We would like to think of these features as being like an image's edge (the outline of a character you want to identify), an ear, a mouth, etc. Now, these features can be anywhere (REALLY IMPORTANT TO REMEMBER THIS STATEMENT), which means we can treat the **feature map** or the **kernel** (let's call it **kernel**) to detect a feature all over an image. What convolution is saying is:\n",
    "\n",
    "> \"Hey, can you like try applying a square detector on every part of an image and see where's there's an ear? Just divide the image into sections to check and find sections where there may be an ear.\"\n",
    "> \n",
    "> ![A visualization of Convolution](https://anhreynolds.com/img/cnn.png)\n",
    "> Image courtesy of [Anh H. Reynolds](https://anhreynolds.com/blogs/cnn.html)\n",
    "\n",
    "\n",
    "\n",
    "Pretty simple right? Okay, let $O(i,j)$ be the output of a Convolution, $I(i,j)$ be the function that grabs pixel value (we will clarify this later), and $K(i, j)$ be the function that grabs from a kernel. Let $m$ and $n$ be the set of things in a kernel.\n",
    "$$\n",
    "O(i,j) = \\sum_{m}\\sum_{n} I(m, n)K(i - m, j - m)\n",
    "$$\n",
    "Now according to the GoodFellow, you can flip it relatively to the kernel. It's commutative! (Yeah prove that Lin). [[2]](#ref2).\n",
    "$$\n",
    "O(i,j) = \\sum_{m}\\sum_{n} I(i - m, j - n)K(m, n)\n",
    "$$\n",
    "But we can just follow the Cross-correlation version according to GoodFellow [[2]](#ref2).\n",
    "$$\n",
    "O(i,j) = \\sum_{m}\\sum_{n} I(i + m, j + n)K(m, n)\n",
    "$$\n",
    "Every input and output could have multiple channels, the input could be an RGB image and thus have three channels, or it could be an output of another layer that has like 64 channels. We would also want to output a 4D tensors that would have multiple channels for further analysis for the model. We denote $l$ to be the kernel index for the corresponding input channels and $k$ be the corresponding index for the output channel.\n",
    "$$\n",
    "O_{k,i,j}= \\sum_{l}\\sum_{m}\\sum_{n} I_{l,i+m,j+n}K_{k,l,m,n}\n",
    "$$\n",
    "## 2.2 Striding and Padding\n",
    "It seems a bit costly to go over all the pixels of an image to find an Ear, after all, the ear is usually in one or two spots of our image. We could get by through applying a kernel every *sth* pixel through striding.\n",
    "$$\n",
    "O_{s,k,i,j}= \\sum_{l}\\sum_{m}\\sum_{n} I_{l,i \\times s+m,j \\times s +n}K_{k,l,m,n}\n",
    "$$\n",
    "The Convolution function **down samples** a matrix, meaning it would have its dimension reduced. We would want to avoid that if we are to make our model complex, by adding more layers on top. If we don't have a way to mitigate aggressive down sampling, we would end up with a pretty small matrix, whose content cannot be used for further analysis by the model. To prevent down sampling, we would implement **padding**, or specifically **zero-padding** by adding zeroes all around the matrix so that we retain some or all of the dimensions.\n",
    "\n",
    "With all the prerequisite for a suitable Convolution Layer explained, we will now implement the **Module** for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef91432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(d2l.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 padding=0, stride=1, lr=0.01, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        self.w = nn.Parameter(torch.normal(0, 0.01, (out_channels, in_channels, kernel_size, kernel_size)))\n",
    "        self.b = nn.Parameter(torch.zeros(out_channels)) if bias else None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # create dimension\n",
    "        p = self.padding\n",
    "        s = self.stride\n",
    "        k = self.kernel_size\n",
    "        \n",
    "        # if padding gt 1\n",
    "        if p > 0:\n",
    "            batch_size, channels, height, width = X.shape\n",
    "            padded_tensor = torch.zeros((batch_size, channels, height + 2 * p, width + 2 * p),\n",
    "                                        dtype=X.dtype, device=X.device)\n",
    "            padded_tensor[:, :, p:height + p, p:width + p] = X\n",
    "            X = padded_tensor\n",
    "            \n",
    "        ## NAIVE IMPLEMENTATION, SUPER SLOW!!\n",
    "        # # Output size calculation\n",
    "        # batch_size, channels, input_height, input_width = X.shape\n",
    "        # output_height = (input_height - k) // s + 1\n",
    "        # output_width = (input_width - k) // s + 1\n",
    "\n",
    "        # # Create the result tensor\n",
    "        # result = torch.zeros((batch_size, self.out_channels, output_height, output_width),\n",
    "        #                      dtype=X.dtype, device=X.device)\n",
    "        \n",
    "        # # Manual loop-based convolution\n",
    "        # for b in range(batch_size):\n",
    "        #     for i in range(output_height):\n",
    "        #         for j in range(output_width):\n",
    "        #             for out_c in range(self.out_channels):\n",
    "        #                 # Extract the slice of the input tensor\n",
    "        #                 input_slice = X[b, :, i * s : i * s + k, j * s : j * s + k]\n",
    "        #                 # Grab the corresponding kernel\n",
    "        #                 kernel = self.w[out_c]\n",
    "        #                 # Perform element-wise multiplication and sum\n",
    "        #                 result[b, out_c, i, j] = (input_slice * kernel).sum()\n",
    "        #                 if self.b is not None:\n",
    "        #                     result[b, out_c, i, j] += self.b[out_c]\n",
    "\n",
    "        # return result\n",
    "            \n",
    "        # unfold X into flattened_kernel_size * patches\n",
    "        unfolded_X = F.unfold(X, kernel_size=(k, k), padding=0, stride=s)\n",
    "        \n",
    "        # unfold weight into out_channel * flattened_kernel_size\n",
    "        unfolded_weight = self.w.view(self.out_channels, -1)\n",
    "        \n",
    "        # Perform matrix multiplication\n",
    "        output_matrix = unfolded_weight @ unfolded_X\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        batch_size, _, input_height, input_width = X.shape\n",
    "        output_height = (input_height - k) // s + 1\n",
    "        output_width = (input_width - k) // s + 1\n",
    "        \n",
    "        # Reshape the output matrix to the correct tensor shape\n",
    "        output_tensor = output_matrix.view(batch_size, self.out_channels, output_height, output_width)\n",
    "\n",
    "        return output_tensor + self.b[None, :, None, None] if self.bias else 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4b880",
   "metadata": {},
   "source": [
    "## 2.3 Pooling\n",
    "In order to stay true to the idea of **translation invariance**, where small changes to the image doesn't affect the overall outcome, a pooling function is needed [[2]](#ref2). We will use **max pooling** that will project a rectangular window throughout the input to find the maximum value. That maximum value can inform the model if a representation appears within a certain part of the input, thus aiding our model in identification and classification.\n",
    "\n",
    "We will now implement a module for this Pooling function. It will be attached after a **Convolution layer** and a **non-linearity layer** (to be discussed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57f9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2d(d2l.Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride if stride is not None else kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Unfold the tensor into patches\n",
    "        unfolded_X = F.unfold(X, kernel_size=self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        \n",
    "        batch_size, _, L = unfolded_X.shape\n",
    "        channels = X.shape[1]\n",
    "    \n",
    "        # unfold x furthers\n",
    "        unfolded_X = unfolded_X.view(\n",
    "            batch_size, channels, self.kernel_size * self.kernel_size, L\n",
    "        )\n",
    "        \n",
    "        # Take max over kernel dimension (not channels)\n",
    "        pooled_X = unfolded_X.max(dim=2)[0]  # shape: (batch_size, channels, L)\n",
    "            \n",
    "        # Get the original dimensions\n",
    "        height, width = X.shape[2], X.shape[3]\n",
    "        \n",
    "        # Correctly calculate output height and width using original dimensions.\n",
    "        output_height = (height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        output_width = (width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        # Reshape the pooled output back to the correct 4D tensor shape.\n",
    "        output = pooled_X.view(batch_size, channels, output_height, output_width)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39693d5d",
   "metadata": {},
   "source": [
    "The final step of our architecture will require a global average pooling function that needs to reduce the whole feature map into a single scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d973316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(d2l.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X.mean(dim=[2, 3], keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304566b",
   "metadata": {},
   "source": [
    "## 2.4 Noise and Non-linearity\n",
    "The introduction of non-linearity makes our model more expressive, allowing it to learn non-linear relationships in our data. Without them, they would be able to learn linear data. A widely-used and modern non-linear function is **ReLU** which is defined as follows\n",
    "$$\n",
    "f(x) = max(0, x)\n",
    "$$\n",
    "By this definition, it means that any input that is negative would be set to 0, otherwise, it would be positive. Krizhevsky showed that it's faster than other saturating methods like tanh [[3]](#ref3) and that a faster converging method like ReLU translates into better Model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1229ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(d2l.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return max(torch.tensor(0.0), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d771e8",
   "metadata": {},
   "source": [
    "With great power, comes great responsibility. The responsibility to reduce overfitting through the injection of noise and randomness, which is what **Dropout** does. We inject randomness by randomly shutting off neuron to see if other neuron plays a greater role in representation, and thus a potential to reduce overfitting and actually generalize to features that matters. Such an activation method helps reduce \"complex co-adaptations of neurons\", which means it could prevent a neuron dependency on other neuron [[3]](#ref3).\n",
    "\n",
    "**Dropout** works by shutting off a neuron with a probability of 0.5, then rescaling the activated neurons to ensure consistent output.\n",
    "\n",
    "Dropout is implemented as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e03511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(d2l.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # if not training\n",
    "        if not self.training:\n",
    "            # return x\n",
    "            return X\n",
    "        \n",
    "        # get dimensions\n",
    "        batch_size, channels, height, width = X.shape\n",
    "        \n",
    "        # create random tensor\n",
    "        indices = torch.rand((batch_size, channels, height, width))\n",
    "\n",
    "        # select all of those where the values are less than the set probability\n",
    "        X = X * (indices > self.p).float()\n",
    "        \n",
    "        # rescale\n",
    "        return X / (1 - self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb1d14",
   "metadata": {},
   "source": [
    "Now if you have read this far my esteemed reader, the best have yet to come. We have finished constructing the building blocks needed for a ResNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562714d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a name=\"ref1\">[1]</a> Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proc. IEEE, vol. 86, no. 11, pp. 2278–2324, Nov. 1998, doi: 10.1109/5.726791.\n",
    "\n",
    "<a name=\"ref2\">[2]</a> I. Goodfellow, Y. Bengio, and A. Courville, “Convolutional Networks,” in Deep Learning. Cambridge, MA, USA: MIT Press, 2016, pp. 321-360.\n",
    "\n",
    "<a name=\"ref3\">[3]</a> Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems 25 (NIPS 2012)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
