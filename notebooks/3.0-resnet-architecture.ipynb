{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef99be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the path of the parent directory (your_project)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import correctly\n",
    "from models.models import Conv2D, MaxPool2d, ReLU, GlobalAvgPool2d, LinearRegression, SoftmaxRegression, SGDFromScratch, CrossEntropyError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916fa48",
   "metadata": {},
   "source": [
    "# 3.0 ResNet Architecture\n",
    "The ResNet architecture introduces the **Residual Learning** block, a fundamental improvement on deeper models. We will talk about the motivation for a deeper neural network, the core problem that hinders it, and how ResNet mitigates this problem. \n",
    "\n",
    "For this notebook, I'd like to quote my Multivariable Calculus professor, I-Shen Lai. Say hello to Professor Lai!\n",
    "\n",
    "> \"Haha... you can quote me if you want.\"\n",
    "> I-Shen Lai, September 3rd 2025.\n",
    "\n",
    "## 3.1 Deep Learning\n",
    "Deep Learning models are made up of multiple sequential layers of other model, usually composed of the models we made in earlier notebooks. The addition of newer layers allow for more representation of complex features, allowing more information to be learned resulting in an even lower loss [[1]](#ref1).\n",
    "\n",
    "However, there is a limit to the number of layers we can effectively add. In practice, when we reach tens of layers, the model faces a **Degradation Problem** that result in stagnant accuracy, accompanied by higher training errors [[1]](#ref1). \n",
    "\n",
    "He et al. proposed that the layers themselves could not fit or approximate to the **identity function**, which is the function that simply return input itself [[1]](#ref1).\n",
    "\n",
    "So if we could somehow skip layers we deem useless or that it would just result in overfitting, then theoretically, we could improve our model, where adding additional layers would not worsen its performance.\n",
    "> *Drum Rolls*\n",
    "\n",
    "## 3.2 Residual Learning Block\n",
    "Residual Learning Block introduced by He has a property of skip connections, in which our layer could either try to learn the underlying function that we desire or the identity function.\n",
    "\n",
    "What is this underlying function? It's the function that model reality and is the ultimate truth. Throughout our lesson, we have been using `LinearRegression` to learn if there's a linear relationship, or we add `ReLU` to model non-linear and complex ones. We define as follows: the input $x$, the underlying function $U(x)$, and the layer's function $F(x)$.\n",
    "\n",
    "$$\n",
    "F(\\mathbf{x}) = U(\\mathbf{x}) - \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Cleverly, He et al. arranged it to the following\n",
    "\n",
    "$$\n",
    "U(\\mathbf{x}) = F(\\mathbf{x}) + \\mathbf{x}\n",
    "$$\n",
    "\n",
    "It's explained that the skip connection is achieved when $F(\\mathbf{x})$ outputs 0, thus achieving $U(\\mathbf{x}) = \\mathbf{x}$, which is the identity function.\n",
    "\n",
    "## 3.3 Batch Normalization\n",
    "Before, we work out the implementation in code. He et al. adopts batch normalization on its architecture, which helps model to converge faster in its training [[2]](#ref2). It must be applied in between affine function and non-linearity function. \n",
    "\n",
    "According to D2L, Batch normalization normalizes a layer's output with the batch's mean $\\hat{u}_\\mathcal{B}$ and standard deviation $\\hat{\\sigma}_\\mathcal{B}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "BN(\\mathbf{x}) &= \\gamma \\odot \\frac{\\mathbf{x} - \\hat{u}_\\mathcal{B}}{\\hat{\\sigma}_\\mathcal{B}} + \\beta \\\\\n",
    "\\hat{u}_\\mathcal{B} &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x} \\\\\n",
    "\\hat{\\sigma}_\\mathcal{B}^2 &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{u}_\\mathcal{B})^2 + \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2eb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following definitions are a direct copy from the d2l, I'm too lazy to work this one out\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "class BatchNorm2d(d2l.Module):    \n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6660d40",
   "metadata": {},
   "source": [
    "## 3.4 ResNet-50 Implementation\n",
    "We will need to design a layer first before constructing our model. A bottleneck design will be utilized since we are going to build a ResNet-50. Each Residual Learning block will consist of a 1x1, 3x3 and 1x1 convolution layer (in that order), which is for down-sampling our features and then upscaling it to the same dimension [[1]](#ref1). For every Convolution layer we added, we add `BatchNorm2d` and `ReLU` for non-linearity and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5baf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(d2l.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.ReLU = ReLU()\n",
    "        \n",
    "        self.conv1 = Conv2D(in_channels, out_channels, kernel_size=1, stride=strides)\n",
    "        self.bn1 = BatchNorm2d(out_channels, 4)\n",
    "        self.conv2 = Conv2D(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = BatchNorm2d(out_channels, 4)\n",
    "        self.conv3 = Conv2D(out_channels, out_channels * 4, kernel_size=1)\n",
    "        self.bn3 = BatchNorm2d(out_channels * 4, 4)\n",
    "        \n",
    "        # Skip connection to match input/output dimensions if needed\n",
    "        if use_1x1conv or in_channels != out_channels * 4:\n",
    "            self.conv4 = Conv2D(in_channels, out_channels * 4, kernel_size=1, stride=strides)\n",
    "            self.bn4 = BatchNorm2d(out_channels * 4, 4)\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "        \n",
    "        self.ReLU = ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = self.ReLU(self.bn1(self.conv1(X)))\n",
    "        Y = self.ReLU(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        \n",
    "        # add skip connection\n",
    "        if self.conv4:\n",
    "            X = self.bn4(self.conv4(X))\n",
    "            \n",
    "        Y += X\n",
    "        \n",
    "        return self.ReLU(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438505b",
   "metadata": {},
   "source": [
    "With the individual layers implemented. We can start building our ResNet-50 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf59f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(d2l.Classifier):\n",
    "    def __init__(self, num_classes, lr):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.bias = True\n",
    "        self.conv1 = Conv2D(kernel_size=7, in_channels=3, out_channels=64, stride=2)\n",
    "        self.pool1 = MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=64, out_channels=64, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=256, out_channels=64, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=256, out_channels=64, use_1x1conv=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=256, out_channels=128, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=512, out_channels=128, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=512, out_channels=128, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=512, out_channels=128, use_1x1conv=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=512, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=1024, out_channels=512, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=2048, out_channels=512, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=2048, out_channels=512, use_1x1conv=True),\n",
    "        )\n",
    "        self.pool2 = GlobalAvgPool2d()\n",
    "        self.fc = LinearRegression(in_features=2048, out_features=1000, lr=self.lr, bias=self.bias)\n",
    "        self.softmax = SoftmaxRegression(1000, num_classes, lr=self.lr, bias=self.bias)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = self.pool1(self.conv1(X))\n",
    "        Y = self.conv2(Y)\n",
    "        Y = self.conv3(Y)\n",
    "        Y = self.conv4(Y)\n",
    "        Y = self.conv5(Y)\n",
    "        Y = self.pool2(Y)\n",
    "        Y = Y.reshape(Y.shape[0], -1)\n",
    "        Y = self.softmax(self.fc(Y))\n",
    "        return Y\n",
    "        \n",
    "        \n",
    "    def loss(self, y_hat, y):\n",
    "        return CrossEntropyError(y_hat, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return SGDFromScratch(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40137e",
   "metadata": {},
   "source": [
    "Let us test the output of this model to verify and see if everything is working correct. We have done a lot of scaffolding and building without testing it out. In the future, we wish to be better by testing at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b401716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0949, 0.0995, 0.0984, 0.0912, 0.1113, 0.1022, 0.0931, 0.1063, 0.0963,\n",
       "         0.1067]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(num_classes=10, lr=0.01)\n",
    "model(torch.randn(1, 3, 228, 228))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ae989",
   "metadata": {},
   "source": [
    "Tada! it works somehow. We have finally finished constructing a ResNet-50 architecture. We can call `eval` to see the shape of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce2d6949",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (259200x49 and 147x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m d2l\u001b[38;5;241m.\u001b[39mFashionMNIST(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, resize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m96\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/d2l/torch.py:285\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/d2l/torch.py:298\u001b[0m, in \u001b[0;36mTrainer.fit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[0;32m--> 298\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/d2l/torch.py:213\u001b[0m, in \u001b[0;36mModule.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m--> 213\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, l, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mResNet50.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 37\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(Y)\n\u001b[1;32m     39\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(Y)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repos/dog_and_cat_classifier_cnn_from_scratch/models/models.py:133\u001b[0m, in \u001b[0;36mConv2D.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    130\u001b[0m unfolded_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Perform matrix multiplication\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m output_matrix \u001b[38;5;241m=\u001b[39m \u001b[43munfolded_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munfolded_X\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Calculate output dimensions\u001b[39;00m\n\u001b[1;32m    136\u001b[0m batch_size, _, input_height, input_width \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (259200x49 and 147x64)"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=10, lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "trainer.fit(model, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785be68",
   "metadata": {},
   "source": [
    "## References\n",
    "<a name=\"ref1\">[1]</a> K. He, C. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778.\n",
    "\n",
    "<a name=\"ref2\">[2]</a> A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, *Dive into Deep Learning*. 2021. [Online]. Available: https://d2l.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
