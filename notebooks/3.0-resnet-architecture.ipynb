{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef99be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "from resnet_from_scratch.model import Conv2D, MaxPool2d, ReLU, GlobalAvgPool2d, LinearRegression, SoftmaxRegression, SGDFromScratch, CrossEntropyError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916fa48",
   "metadata": {},
   "source": [
    "# 3.0 ResNet Architecture\n",
    "The ResNet architecture introduces the **Residual Learning** block, a fundamental improvement on deeper models. We will talk about the motivation for a deeper neural network, the core problem that hinders it, and how ResNet mitigates this problem. \n",
    "\n",
    "For this notebook, I'd like to quote my Multivariable Calculus professor, I-Shen Lai. Say hello to Professor Lai!\n",
    "\n",
    "> \"Haha... you can quote me if you want.\"\n",
    "> I-Shen Lai, September 3rd 2025.\n",
    "\n",
    "## 3.1 Deep Learning\n",
    "Deep Learning models are made up of multiple sequential layers of other model, usually composed of the models we made in earlier notebooks. The addition of newer layers allow for more representation of complex features, allowing more information to be learned resulting in an even lower loss [[1]](#ref1).\n",
    "\n",
    "However, there is a limit to the number of layers we can effectively add. In practice, when we reach tens of layers, the model faces a **Degradation Problem** that result in stagnant accuracy, accompanied by higher training errors [[1]](#ref1). \n",
    "\n",
    "He et al. proposed that the layers themselves could not fit or approximate to the **identity function**, which is the function that simply return input itself [[1]](#ref1).\n",
    "\n",
    "So if we could somehow skip layers we deem useless or that it would just result in overfitting, then theoretically, we could improve our model, where adding additional layers would not worsen its performance.\n",
    "> *Drum Rolls*\n",
    "\n",
    "## 3.2 Residual Learning Block\n",
    "Residual Learning Block introduced by He has a property of skip connections, in which our layer could either try to learn the underlying function that we desire or the identity function.\n",
    "\n",
    "What is this underlying function? It's the function that model reality and is the ultimate truth. Throughout our lesson, we have been using `LinearRegression` to learn if there's a linear relationship, or we add `ReLU` to model non-linear and complex ones. We define as follows: the input $x$, the underlying function $U(x)$, and the layer's function $F(x)$.\n",
    "\n",
    "$$\n",
    "F(\\mathbf{x}) = U(\\mathbf{x}) - \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Cleverly, He et al. arranged it to the following\n",
    "\n",
    "$$\n",
    "U(\\mathbf{x}) = F(\\mathbf{x}) + \\mathbf{x}\n",
    "$$\n",
    "\n",
    "It's explained that the skip connection is achieved when $F(\\mathbf{x})$ outputs 0, thus achieving $U(\\mathbf{x}) = \\mathbf{x}$, which is the identity function.\n",
    "\n",
    "## 3.3 Batch Normalization\n",
    "Before, we work out the implementation in code. He et al. adopts batch normalization on its architecture, which helps model to converge faster in its training [[2]](#ref2). It must be applied in between affine function and non-linearity function. \n",
    "\n",
    "According to D2L, Batch normalization normalizes a layer's output with the batch's mean $\\hat{u}_\\mathcal{B}$ and standard deviation $\\hat{\\sigma}_\\mathcal{B}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "BN(\\mathbf{x}) &= \\gamma \\odot \\frac{\\mathbf{x} - \\hat{u}_\\mathcal{B}}{\\hat{\\sigma}_\\mathcal{B}} + \\beta \\\\\n",
    "\\hat{u}_\\mathcal{B} &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x} \\\\\n",
    "\\hat{\\sigma}_\\mathcal{B}^2 &= \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{u}_\\mathcal{B})^2 + \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2eb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following definitions are a direct copy from the d2l, I'm too lazy to work this one out\n",
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "class BatchNorm2d(d2l.Module):    \n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6660d40",
   "metadata": {},
   "source": [
    "## 3.4 ResNet-50 Implementation\n",
    "We will need to design a layer first before constructing our model. A bottleneck design will be utilized since we are going to build a ResNet-50. Each Residual Learning block will consist of a 1x1, 3x3 and 1x1 convolution layer (in that order), which is for down-sampling our features and then upscaling it to the same dimension [[1]](#ref1). For every Convolution layer we added, we add `BatchNorm2d` and `ReLU` for non-linearity and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5baf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(d2l.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.ReLU = ReLU()\n",
    "        \n",
    "        self.conv1 = Conv2D(in_channels, out_channels, kernel_size=1, stride=strides)\n",
    "        self.bn1 = BatchNorm2d(out_channels, 4)\n",
    "        self.conv2 = Conv2D(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = BatchNorm2d(out_channels, 4)\n",
    "        self.conv3 = Conv2D(out_channels, out_channels * 4, kernel_size=1)\n",
    "        self.bn3 = BatchNorm2d(out_channels * 4, 4)\n",
    "        \n",
    "        # Skip connection to match input/output dimensions if needed\n",
    "        if use_1x1conv or in_channels != out_channels * 4:\n",
    "            self.conv4 = Conv2D(in_channels, out_channels * 4, kernel_size=1, stride=strides)\n",
    "            self.bn4 = BatchNorm2d(out_channels * 4, 4)\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "        \n",
    "        self.ReLU = ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = self.ReLU(self.bn1(self.conv1(X)))\n",
    "        Y = self.ReLU(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        \n",
    "        # add skip connection\n",
    "        if self.conv4:\n",
    "            X = self.bn4(self.conv4(X))\n",
    "            \n",
    "        Y += X\n",
    "        \n",
    "        return self.ReLU(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e438505b",
   "metadata": {},
   "source": [
    "With the individual layers implemented. We can start building our ResNet-50 model. For smaller ResNet models though, we would use a normal design with just two Convolution layer.\n",
    "\n",
    "The ResNet-50 is implemented below. Marvel at how simple it looks! (or not, if you have been following our notebooks series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf59f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(d2l.Classifier):\n",
    "    def __init__(self, num_classes, lr,  in_channels=1):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.bias = True\n",
    "        self.conv1 = Conv2D(kernel_size=7, in_channels=in_channels, out_channels=64, stride=2)\n",
    "        self.pool1 = MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=64, out_channels=64, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=256, out_channels=64, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=256, out_channels=64, use_1x1conv=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=256, out_channels=128, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=512, out_channels=128, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=512, out_channels=128, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=512, out_channels=128, use_1x1conv=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=512, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=1024, out_channels=256, use_1x1conv=True),\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            ResNetLayer(in_channels=1024, out_channels=512, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=2048, out_channels=512, use_1x1conv=True),\n",
    "            ResNetLayer(in_channels=2048, out_channels=512, use_1x1conv=True),\n",
    "        )\n",
    "        self.pool2 = GlobalAvgPool2d()\n",
    "        self.fc = LinearRegression(in_features=2048, out_features=1000, lr=self.lr, bias=self.bias)\n",
    "        self.softmax = SoftmaxRegression(1000, num_classes, lr=self.lr, bias=self.bias)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = self.pool1(self.conv1(X))\n",
    "        Y = self.conv2(Y)\n",
    "        Y = self.conv3(Y)\n",
    "        Y = self.conv4(Y)\n",
    "        Y = self.conv5(Y)\n",
    "        Y = self.pool2(Y)\n",
    "        Y = Y.reshape(Y.shape[0], -1)\n",
    "        Y = self.softmax(self.fc(Y))\n",
    "        return Y\n",
    "        \n",
    "        \n",
    "    def loss(self, y_hat, y):\n",
    "        return CrossEntropyError(y_hat, y)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return SGDFromScratch(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40137e",
   "metadata": {},
   "source": [
    "This is a huge step-up from a single Linear model in our first notebook. Back in the days of Lin's, people would handwrite (or type) this with more, *cough* *cough*, archaic language like C++, ASM, Fortran or even Lisp (totally didn't rip this off from Google's AI Overview). The advent of tooling, like PyTorch and other ML/AI libraries, move us away from our concern of implementing each functions correctly to dealing with them in a high-level way--You wouldn't have to worry if \n",
    "\n",
    "Let us test the output of this model to verify and see if everything is working correct. We have done a lot of scaffolding and building without testing it out. In the future, we wish to be better by testing at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b401716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1070, 0.0947, 0.0928, 0.1102, 0.1057, 0.0815, 0.1008, 0.1075, 0.0957,\n",
       "         0.1040]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet50(num_classes=10, in_channels=3, lr=0.01)\n",
    "model(torch.randn(1, 3, 228, 228))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785be68",
   "metadata": {},
   "source": [
    "## References\n",
    "<a name=\"ref1\">[1]</a> K. He, C. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778.\n",
    "\n",
    "<a name=\"ref2\">[2]</a> A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, *Dive into Deep Learning*. 2021. [Online]. Available: https://d2l.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
